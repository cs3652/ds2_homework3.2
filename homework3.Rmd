---
title: "Homework 3"
author: "Chirag Shah"
date: '2019-04-06'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Part a

```{r}
library(ISLR)
library(caret)
library(glmnet)
library(MASS)
library(e1071)
library(mlbench)
library(pROC)
library(AppliedPredictiveModeling)

summary(Weekly)
```

## Plotting Graphical Summaries

```{r}
#simple plot with Lag1 as the predictor (can be done for the other Lag variables)
plot(Today~Lag1, col = "red", data = Weekly)
simplelm = lm(Today~Lag1, data = Weekly)
abline(simplelm, lwd = 3, col = "grey")

#simple plot with Volume as the predictor
plot(Today~Volume, col = "blue", data = Weekly)
simplelm2 = lm(Today~Volume, data = Weekly)
abline(simplelm2, lwd = 3, col = "grey")

#simple plot with Year as the predictor
plot(Today~Year, col = "green", data = Weekly)
simplelm3 = lm(Today~Year, data = Weekly)
abline(simplelm3, lwd = 3, col = "grey")
```

Part b 

```{r}
logmod <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, family = "binomial", data = Weekly)
summary(logmod)
```

Of the 6 predictors in the model, the only predictor that was statistically significant was Lag2. The values from 2 weeks before a current values significantly predicts direction. Because our model is a logistic model, the exponentiated coefficient will give us the the change in odds ratio. An increase of 1 in Lag2 increases the odds of direction by e^0.058 = 1.06 compared to the original odds.

Part c

```{r}
#Creating confusion matrix
prob = predict(logmod, type = "response")
pred = rep("Down", 1089)
pred[prob > 0.5] = "Up"
confusionMatrix(data = as.factor(pred), reference = Weekly$Direction, positive = "Down")
```

Overall Fraction of Correct Predictions: 611/1089 = 0.5611

Based on the confusion matrix, we can see that the prediction shows that most of the cases go up (i.e. 987/1089 cases) whereas in reality there are only 605/1089 that go up. This indicates that our prediction model does a poor job of predicting direction. In light of this, we can find a very large proportion of true positives from our model (i.e. that are truly UP: 557/605 = 0.921) at the cost/tradeoff of finding many false positives (i.e. that we have falsely predicted as UP: 430/987 = 0.436). 

Part d

```{r}
test.pred.prob  <- predict(logmod, newdata = Weekly, type = "response")

roc.glm <- roc(Weekly$Direction, test.pred.prob)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)
```

The AUC is 0.554.

Part e

```{r}
training.data = Weekly[Weekly$Year < 2009,]
test.data = Weekly[Weekly$Year > 2008,]

logmod2 = glm(Direction~Lag1+Lag2, data = training.data, family = "binomial")
summary(logmod2)
```

```{r}
test.pred.prob2 <- predict(logmod2, newdata = test.data, type = "response")

roc.glm2 <- roc(test.data$Direction, test.pred.prob2)
plot(roc.glm2, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm2), col = 4, add = TRUE)
```

The AUC is 0.556.

Part f

```{r}
#LDA
lda.fit <- lda(Direction~Lag1+Lag2, data = training.data)
plot(lda.fit)
```

```{r}
#LDA ROC Curve
lda.pred <- predict(lda.fit, newdata = test.data, type = "prob")
head(lda.pred$posterior)

roc.lda <- roc(test.data$Direction, lda.pred$posterior[,2])

plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.lda), col = 4, add = TRUE)
```

The AUC value for the LDA model is 0.557.

```{r}
#QDA
qda.fit <- qda(Direction~Lag1+Lag2, data = training.data)
qda.fit
```

```{r}
#QDA ROC Curve
qda.pred <- predict(qda.fit, newdata = test.data, type = "prob")
head(qda.pred$posterior)

roc.qda <- roc(test.data$Direction, qda.pred$posterior[,2])

plot(roc.qda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.qda), col = 4, add = TRUE)
```

The AUC for the QDA is 0.529. 

Part g

```{r}
training.data2 = (Weekly$Year < 2009)
test.data2 = (Weekly$Year > 2008)
train_weekly = Weekly[!training.data2,2:3]
train_direction = Weekly$Direction[!training.data2]

ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(1)
model.knn <- train(x = Weekly[training.data2,2:3],
                   y = Weekly$Direction[training.data2],
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1, 50, by = 2)),
                   trControl = ctrl,
                   metric = 'ROC')
ggplot(model.knn)
```

```{r}
#KNN ROC Curve
knn.pred <- predict(model.knn, newdata = train_weekly, type = "prob")[,2]
roc.knn <- roc(train_direction, knn.pred)
plot(roc.knn, legacy.axes = TRUE, print.auc = TRUE)
```

The AUC for the KNN is 0.545. 

Given that the LDA had the greatest AUC value (0.557), that is the model that has the greatest probability of getting the prediction correct (i.e. the actual value in the dataset). The other models had similar AUC values but were slightly less. 